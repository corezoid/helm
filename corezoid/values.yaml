global:
  # second level domain only! / core domain (example: corezoid.com)
  domain: "devops.corezoid.com"
  # subdomain / sitename - dev.mpo.corezoid.com (example: dev.mpo)
  subdomain: "corezoid-k8s-public"

  # docker image registry
  imageRegistry: "docker-hub.middleware.biz"
  #IfNotPresent | Always | Never
  imagePullPolicy: "IfNotPresent"
  repotype: "public"
  product: "corezoid"
  centos8Repo: public-corezoid-centos8
  useCentos8: false
  # Ingress annotations
  ingress:
    annotations:
      kubernetes.io/ingress.class: "nginx"

#      kubernetes.io/ingress.class: alb
#      alb.ingress.kubernetes.io/scheme: internet-facing
#      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:eu-west-1:305168114386:certificate/5fad3576-220b-4c69-ac85-e56b3fe684bd
#      alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-2-Ext-2018-06
#      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
#      alb.ingress.kubernetes.io/load-balancer-attributes: "routing.http2.enabled=false"

      # access restriction
      # alb.ingress.kubernetes.io/inbound-cidrs: 8.8.8.8/32, 1.1.1.1/32, 0.0.0.0/0
      # for creating route53 record-set
      # external-dns.alpha.kubernetes.io/hostname: {{ .Values.global.subdomain}}.{{ .Values.global.domain }}

  ###########################################
  ######## Settings for filesystems #########
  # Define global storage class: efs / nfs / manual (manual storage should support accessModes: - ReadWriteMany)

  storage: nfs
  # Define global storageClass name
  storageClassName: "corezoid-storage-class-default"
  ######## Settings AWS EFS filesystem   ########
  efs:
    awsRegion: "eu-west-1"
    efsFileSystemId: "fs-3de75409"
    ## set true if you choose storage: efs
    enabled: false
  ######## Settings NFS filesystem   ########
  nfs:
    ## set true if you choose storage: nfs
    enabled: true


  ##############################################
  ######## NetworkPolicy########################
  networkPolicy:
    enabled: false
    labelsSelector:
      ingress:
        namespaceSelector:
          # Selector for ingress namespace
          name: alb-ingress-nginx-internal
        podSelector:
          # Selector for ingress deploymnets
          app.kubernetes.io/name: ingress-nginx
      monitoring:
        # Selector for prometheus namespace
        namespaceSelector:
          name: monitoring
        # Selector for prometheus deploymnets
        podSelector:
          release: prometheus-stack
  ########  Settings for stateful services  ########

  #######  PostgreSQL  ########
  ## Supported version PostgreSQL-13.*
  ## for RDS minimum instance - db.t2.medium / master user set "postgres"
  db:
    # if internal true - create and use internal postgres container
    # if internal false - enable external db, like aws rds
    internal: true
    ## secret configuration for postgresql

    ## pvc name
    persistantVolumeClaimName: "postgresql-pvc"
    secret:
      ## true - secret will be created automaticaly with provided values
      ## false - secret should be created manualy
      create: true
      ## secret name
      name: "postgresql-secret"
      ## for init db and roles dbsuperuser and dbuser cannot be changed (because used as plain text in sql)
      dbsuperuser: "postgres"
      ## password - for dbsuperuser
      dbsuperuserpwd: "Corezoid1234!b"
      data:
        #dbhost: "postgresql.postgresql.svc.cluster.local"
        dbhost: "pg-postgresql" # if internal=true
        dbport: "5432"
        dbuser: "internal_user"
        dbpwd: "8hhgasdgiufghrtrnr4"
        dbuser_fdw: "fdw_user"
        dbpwd_fdw: "fdw_user_passwd123"

    ###### ATENTION! shards_count and shards cant be changed after first init!!!! ####
    # be careful in defining this variable
    # count of shards created in psql, if unset, default - 10
    shards_count: 10
    # count of shards created in psql, if unset, default - 10 - in array format
    shards: ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]

    # set false to disable if you don't want to use PGBouncer recommended - true
    bouncer: true
    bouncer_minReplicas: "2"
    bouncer_maxReplicas: "6"
    bouncer_resources:
      limits:
        cpu: "500m"
        memory: "500Mi"
      requests:
        cpu: "500m"
        memory: "500Mi"
    bouncer_affinity: {}
    #    # hard AZ-based affinity
    #      podAntiAffinity:
    #        requiredDuringSchedulingIgnoredDuringExecution:
    #          - labelSelector:
    #              matchExpressions:
    #              - key: tier
    #                operator: In
    #                values:
    #                  - pgbouncer
    #            topologyKey: failure-domain.beta.kubernetes.io/zone
    #    # soft AZ-based affinity
    #      podAntiAffinity:
    #        preferredDuringSchedulingIgnoredDuringExecution:
    #          - labelSelector:
    #              matchExpressions:
    #                - key: tier
    #                  operator: In
    #                  values:
    #                    - pgbouncer
    #            topologyKey: failure-domain.beta.kubernetes.io/zone

    # Maximum number of client connections allowed.
    maxclientconn: "100000"
    # How many server connections to allow per user/database pair. Can be overridden in the per-database configuration.
    default_pool_size: "50"
    # Maximum number of client connections allowed
    maxDBConnections: "1000"
    # Do not allow more than this many server connections per user (regardless of database)
    maxUserConnections: "1000"
    # How many additional connections to allow to a pool
    reserve_pool_size: "5"
    # If a client has not been serviced in this many seconds, use additional connections from the reserve pool
    reservePoolTimeout: "2"
    # The pooler will close an unused server connection that has been connected longer than this
    serverLifetime: "1200"
    # If a server connection has been idle more than this many seconds it will be dropped
    serverIdleTimeout: "15"
    # If connection and login wonâ€™t finish in this amount of time, the connection will be closed.
    serverConnectTimeout: "15"

    # Server is released back to pool after transaction finishes. default: session, see man https://wiki.postgresql.org/wiki/PgBouncer
    # transaction | session
    default_pool_mode: transaction

    # Postgres corezoid schema version
    postgres_schema:
      version: "5.10.0.4"
      db_schema_rds: true  # if RDS base then set to true, if standard PostgreSQL then false

    rotation:
      ## enabled: true or false
      enabled: true
      # tasks_archive table cleanup period in crontab format
      # minute hour day month dayofweek
      schedule: "0 0 1 * *"

  #######  Redis  ########
  ## Supported version  from > 3.2.4 - 3.2.12

  ## NO CLUSTER Mode!
  redis:
    # if internal true - create and use internal k8s redis container
    # if internal false - enable external redis, like aws elasticache (Engine Version Compatibility: 3.2.10)
    internal: false
    ## pvc name if it already exist or was created manualy
    persistantVolumeClaimName: "redis-pvc"
    ## secret configuration for redis
    sentinel:
      enable: true
      master_name: "mymaster"
    secret:
      ## true - secret will be created automatically with provided values
      ## false - secret should be created manually
      create: true
      name: "redis-secret"
      # you can specify different servers for redis for counters, cache and timers  - or specify the same server in variables
      data:
        host: "redis-sentinel.redis-sentinel.svc.cluster.local"
        port: "26379"
        password: "redispassword"
        # default redis for cache
        host_cache: "redis-sentinel.redis-sentinel.svc.cluster.local"
        port_cache: "26379"
        password_cache: "redispassword"
        # default redis for timers
        host_timers: "redis-sentinel.redis-sentinel.svc.cluster.local"
        port_timers: "26379"
        password_timers: "redispassword"



  #######  Elasticsearch  ########
  ## Supported version 7.17.*
  elasticsearch:
    tag: "7.17.5"
    # if internal true - create and use internal elasticsearch container
    # if internal false - enable external elasticsearch, like aws elasticsearch service
    internal: false
    ## secret configuration for elasticsearch
    secret:
      ## true - secret will be created automaticaly with provided values
      ## false - secret should be created manualy
      create: true
      name: "elasticsearch-secret"
      data:
        host: "elasticsearch-service.elasticsearch.svc.cluster.local"
        #host: "elasticsearch-service" # if internal=true
        ## be careful with port - in internal install used 9200, in aws uses 443
        port: "9200"
        schema: "http"
      # Optional
      auth_enabled: false
      auth_data:
        username: ""
        password: ""


  #######  RabbitMQ  ########
  ## Supported version  3.9.*
  mq:
    # if internal true - create and use internal rmq container
    # if internal false - enable external rmq (on aws ec2 instances as example)
    internal: false
    ## secret configuration for rabbitmq
    secret:
      ## true - secret will be created automatically with provided values
      ## false - secret should be created manually
      create: true
      name: "rabbitmq-secret"
      data:
        host: "rabbitmq.rabbitmq.svc.cluster.local"
#        host: "rabbit-service" # if internal=true
        port: "5672"
        vhost: "/conveyor"
        username: "rabbituser"
        password: "vefgemknvlwkem"
    # vm_memory_high_watermark.relative
    memoryHighWatermark: 0.3

  ########  Settings for Corezoid services  ########
  use_limits_from_server: false
  ## Settings for API Corezoid
  capi:
    tag: "7.12.0.2"
    minReplicas: 2
    maxReplicas: 4
    resources:
      limits:
        cpu: 2
        memory: 2000Mi
      requests:
        cpu: 500m
        memory: 500Mi
    ## Allows you to see the api keys of other users in one company
    share_api_keys_in_company: "false"
    ## Init password for admin@corezoid.loc
    init_admin_password: "Q10J-H(QY95jf3zuU463N"
    ## restricting user registration only within the domains specified below if enabled
    registration_restriction:
      enable: false
      allowed_domains:
        - "gmail.com"
        - "yahoo.com"
    check_2fa: true
    # Enable cookie secure for api
    secure: true
    # Auth provider
    auth_providers_enable: false
    capi_saml_secret_name: "capi-saml-secret"

    # User requests per second limits
    user_limits_interface: "120"  # other requests (anything that is not create/modify task)
    user_limits_max_user_rate: "100" # RPS in the environment

    # SAML SSO configuration for multiple IdP
    saml_idp:
#      # Sample SAML configuration for login page button SSO
#      sso:
#        name: "Google SAML"
#        front_button_show: true
#        login_path: "/auth2/saml/sso"
#        icon_url: "https://upload.wikimedia.org/wikipedia/commons/archive/5/53/20210618182605%21Google_%22G%22_Logo.svg"
#        idp_signs_envelopes: false
#        idp_signs_assertions: true
#        idp_signs_metadata: false
#        sp_consume_url: "/auth2/saml/sso/return"
#        # IdP SAML Metadata file content for SSO authentication
#        auth_metadata_content: ""
      # Sample SAML Support configuration for /support endpoint
      support:
        name: "Google SAML2"
        front_button_show: true
        login_path: "/auth2/saml/support"
        icon_url: "https://upload.wikimedia.org/wikipedia/commons/archive/5/53/20210618182605%21Google_%22G%22_Logo.svg"
        idp_signs_envelopes: false
        idp_signs_assertions: true
        idp_signs_metadata: false
        sp_consume_url: "/auth2/saml/support/return"
        # IdP SAML Metadata file content for SSO authentication
        auth_metadata_content:
          <?xml version="1.0" encoding="UTF-8"?>
          <EntityDescriptor
          .................
          </EntityDescriptor>

    # Ð¡aptcha when login in UI enable/disable
    capi_front_captcha_disabled: true
    # after fail  login_attempts show capcha or block ( can choose in superadmin )
    login_attempts: "2"
    #captcha_provider - recaptcha or hcaptcha, default recaptcha
    captcha_provider: "recaptcha"
    captcha_verify_url: "https://www.google.com/recaptcha/api/siteverify"
    secret:
      ## true - secret will be created automatically with provided values
      ## false - secret should be created manually
      create: true
      name: "capi-secret"
      data:
        # Key for captcha when login in UI
        capi_front_captcha_key: ""
        # Secret for captcha
        capi_backend_captcha_key: ""
    # max req/sec of create|modify|delete for conv|folder|dashboard in 1 sec
    max_reqs_limit: 50000
    # merchant_api settings
    merchant_api:
      skip_otp: true
    # cookie expire time in sec (default 900)
    cookie_expr_time: 7200
    # For create company. We recommend using corezoid_internal for the new installation, if the old installation then use only mapi
    companies_manager: "mapi"
    ## Setting for configure frontend
    front_setting:
      # documentation (navigate by clicking on DOCS link)
      doc_host: "doc.corezoid.com"
      doc_index: "/"
      ui:
        # enabled Market button
        market: false
        # button Create -> Bot platform
        bot_platform: false
        # enabled merge by default
        features: true
        # billing button display
        billing: false
        # display of the git_call button
        git_call: false
        # set default company name
        default_company: "My Corezoid"
        # Tab name
        tab_name: "Corezoid"
        # disable or enable logo on main page
        disabled_auth_logo: false
        # colors
        color_main: "#0791e5"
        color_logo: "#0791e5"
        color_logo_hover: "#056cab"
    ldap_server: "ldap.middleware.loc"
    ldap_port: "389"
    ldap_base: "ou=people,o=middleware"
    ldap_filter: "uid"
    # then this param is true, bind_user_name, bind_user_pass should be filled. if it's false it is not necessary
    ldap_first_bind_user: true
    ldap_bind_user_name: "uid=ldap,ou=people,o=middleware"
    ldap_bind_user_pass: ""
    ldap_user_nick_entry: "cn"
    ldap_tls: false
    # max task size for process conv
    max_task_size_for_process_conv: 264000
    # max task size for state diagramm conv
    max_task_size_for_st_diagramm_conv: 264000
    logic_settings:
      # max allowed threads for api logic
      api_max_thread: 50000
      sender_max_threads: 25
      timer_default: 1 # 30
    # enigma private_key_id
    enigma_pk_id: ""
    # clean trash config
    scraper:
      scrap_interval: "1"
      ttl: "30"


    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - capi
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - capi
  #            topologyKey: failure-domain.beta.kubernetes.io/zone


  ########  Settings for services  ########

  # http-worker
  http:
    tag: "3.11.0.5"
    minReplicas: 2
    maxReplicas: 4
    resources:
      limits:
        cpu: 2
        memory: 2000Mi
      requests:
        cpu: 1
        memory: 500Mi
    max_keep_alive_connections_len: "0"
    max_http_resp_size: "5242880"
    blocked_domains:
      - "169.254.169.254"
      - "mail.ru"
    tune:
      http_consumer_queues_count: 8
      ## The specified number of tcp connections will be created for each queue
      http_consumer_connections_per_queue: 4
      ## Virtual connections for one connections_per_queue
      http_consumer_channels_per_connection: 4
      ## The number of queues within the channel
      http_consumer_messages_prefetch_size_per_channel: 10
      consumer_settings_messages_prefetch_size_per_channel: 50
      ## The number of queues within the channel
      consumer_response_prefetch_size_per_channel: 50

    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - http-worker
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - http-worker
  #            topologyKey: failure-domain.beta.kubernetes.io/zone

  # usercode
  usercode:
    tag: "7.4.0"
    minReplicas: 2
    maxReplicas: 4
    # application resources limits and requests
    resources:
      limits:
        cpu: 4
        memory: 2000Mi
      requests:
        cpu: 1
        memory: 1000Mi
    # enigma private_key_id
    enigma_pk_id: ""
    # max time for each execution of code, kill process after this time
    max_exec_code_time: 1000

    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - usercode
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - usercode
  #            topologyKey: failure-domain.beta.kubernetes.io/zone

  # conveyor-worker
  worker:
    tag: "4.11.0.1"
    minReplicas: 2
    maxReplicas: 4
    # application resources limits and requests
    resources:
      limits:
        cpu: 2
        memory: 2000M
      requests:
        cpu: 1
        memory: 500M
    # enigma private_key_id
    write_data_to_history: false
    enigma_pk_id: ""
    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - worker
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - worker
  #            topologyKey: failure-domain.beta.kubernetes.io/zone

  # mult
  mult:
    tag: "2.11.0.1"
    minReplicas: 2
    maxReplicas: 4
    ## pvc name if it already exist or was created manualy
    persistantVolumeClaimName: "mult-pvc"
    ## true if pvc for mult should be created automaticaly
    ## false if you already have pvc
    persistantVolumeClaimCreate: true
    # Support for explicit storage scheduler (eg: Stork)
    schedulerName: ""
    # application resources limits and requests
    resources:
      limits:
        cpu: 4
        memory: 4000Mi
      requests:
        cpu: 1
        memory: 500Mi
    # enigma private_key_id
    enigma_pk_id: ""

    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - mult
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - mult
  #            topologyKey: failure-domain.beta.kubernetes.io/zone

  # webadm
  webadm:
    tag: "5.11.3"
    minReplicas: 1
    resources:
      limits:
        cpu: 200m
        memory: 500Mi
      requests:
        cpu: 200m
        memory: 500Mi
    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - corezoid-web-adm
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - corezoid-web-adm
  #            topologyKey: failure-domain.beta.kubernetes.io/zone

  # syncapi
  syncapi:
    subdomain: "syncapi-k8s-public"
    tag: "2.3.6"
    minReplicas: 2
    maxReplicas: 4
    # Max timeout to receive callbacks (milliseconds)
    callback_max_timeout: "60000"
    resources:
      limits:
        cpu: 2
        memory: 2000Mi
      requests:
        cpu: 1
        memory: 500Mi

    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - syncapi
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - syncapi
  #            topologyKey: failure-domain.beta.kubernetes.io/zone

  # merchant
  merchant:
    tag: "v0.0.27.3"
    secret:
      ## true - secret will be created automaticaly with provided values
      ## false - secret should be created manualy
      create: true
      name: "merchant-secret"
      data:
        # login/secret for corezoid-merchant communications
        merchant_login: apiLogin1
        merchant_secret: I9EmB170XUbe
    minReplicas: 1
    maxReplicas: 1
    # application resources limits and requests
    resources:
      limits:
        cpu: 500m
        memory: 800Mi
      requests:
        cpu: 100m
        memory: 500Mi

  # enigma global settings
  enigma:
    enabled: false
    encryption: false
    key_manager:
      tag: "1.1.0"
      aws_kms_role: ""
      minReplicas: 1
      maxReplicas: 1
      resources:
        limits:
          cpu: 1000m
          memory: 1300Mi
        requests:
          cpu: 350m
          memory: 500Mi

  # single-account
  sa:
    # true or false
    enabled: false
    google_client_id: ""
    google_client_secret: ""
  sa_web:
    subdomain: ""

  # gitcall | worker and rabbit settings
  gitcall:
    # true or false
    enabled: false
    mq_vhost: "/gitcall"
    dunder_mq_vhost: "/dundergitcall"

  dbcall:
    enabled: false
    host: "dbcall-k8s-public.devops.corezoid.com"
    schema: "https"



  # conf-agent-server
  conf_agent_server:
    tag: "1.9.1"
    minReplicas: 1
    maxReplicas: 4
    resources:
      limits:
        cpu: 1000m
        memory: 1000Mi
      requests:
        cpu: 1000m
        memory: 1000Mi
    affinity: {}
  #    # hard AZ-based affinity
  #      podAntiAffinity:
  #        requiredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #              - key: tier
  #                operator: In
  #                values:
  #                  - conf-agent-server
  #            topologyKey: failure-domain.beta.kubernetes.io/zone
  #    # soft AZ-based affinity
  #      podAntiAffinity:
  #        preferredDuringSchedulingIgnoredDuringExecution:
  #          - labelSelector:
  #              matchExpressions:
  #                - key: tier
  #                  operator: In
  #                  values:
  #                    - conf-agent-server
  #            topologyKey: failure-domain.beta.kubernetes.io/zone


  # web-superadm
  web_superadm:
    tag: "1.7.1"
    subdomain: "superadm-k8s-public"
    minReplicas: 1

  limits:
    tag: "1.4.1"
    minReplicas: 2

  prometheus_metrics: false

  # global log level definition, info - default
  # available value info | warning | error | debug
  log_level: debug
  # allows store crushdumps and send to Telegram chat
  store_dumps:
    enabled: false
    dumps_dir: /var/dumps/
    http_endpoint: http://dump-service:3000/upload
    pvc_size: 10Gi
    notification_url: http://dump-service:3000/telegram
    # domain name for dumps web ui
    subdomain: ""
    #    dump-k8s-public
    # how to create login and password
    # https://www.web2generators.com/apache-tools/htpasswd-generator
    base_auth:
      - admin:$apr1$kdvn7ybj$Wp1TO4qCvlTpUu1x248Nb0
    # How to create telegram bot and get chat ID
    # https://core.telegram.org/bots
    # To get chat id replace {BOT_TOKEN} and open link in a browser
    # https://api.telegram.org/bot{BOT_TOKEN}/getUpdates
    # telegram token
    telegram_token: ""
    # telegram chat id
    telegram_group: ""
    rotate:
      # dumps rotate interval (hourly|daily|weekly|monthly|yearly)
      interval: daily
      # number of rotation
      copies: 7

