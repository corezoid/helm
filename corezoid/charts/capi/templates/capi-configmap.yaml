apiVersion: v1
data:
  {{ .Values.appName }}.config: |
    %% -*- mode: erlang;  -*-
    [

        {corezoid_license_client, [
          {path_to_license, "/ebsmnt/certs/corezoid_license"}
        ]},

        {corezoid_global_stats, [
            {disabled, true}
        ]},

        %% for clustering components
        {corezoid_cluster, [
            {backend, redis}, %% maybe if future list will increase
            {redis, [
            {{- if eq .Values.global.redis.internal false }}
                {host, "${REDIS_HOST}"},
                {port, ${REDIS_PORT}},
                {password,"${REDIS_PASSWORD}"},
            {{- else }}
                {host, "redis-master"},
                {port, 6379},
                {password,""},
            {{- end }}
                {database, 10}
            ]}
        ]},

        {is_ready, [
          {port, 8383},
          {disabled, false}
        ]},

        %% needed for conf_agent
        {corezoid_sdk, [
          {host, "{{ .Values.global.subdomain }}.{{ .Values.global.domain }}"},
          {scheme, "https://"}
        ]},

        {conf_agent, [
         {pgsql, [
          {{- if .Values.global.db.bouncer }}
            {host, "pgbouncer-service"},
          {{- else }}
            {host, "${POSTGRES_DBHOST}"},
          {{- end }}
           {user, "${POSTGRES_DBUSER}"},
           {dbname, "settings"},
           {password, "${POSTGRES_DBPWD}"},
           {min_size, 10},
           {max_size, 10},
           {start_size, 1}
         ]},

         {port, 8585},
         {is_admin, true},

        {publish_request, [
                    {servers, [
      {{- if eq .Values.global.mq.internal false }}
                  [
                    {host, "${MQ_HOST}"},
                    {port, ${MQ_PORT}},
                    {username, <<"${MQ_USERNAME}">>},
                    {password, <<"${MQ_PASSWORD}">>},
                    {vhost, <<"${MQ_VHOST}">>}
                ]
      {{- else }}
                  [
                    {host, "rabbit-service"},
                    {port, 5672},
                    {username, <<"${MQ_USERNAME}">>},
                    {password, <<"${MQ_PASSWORD}">>},
                    {vhost, <<"${MQ_VHOST}">>}
                ]
      {{- end }}
            ]},
        {queues_count, 1},
        {min_size, 1},
        {max_size, 1},
        {start_size, 1}
        ]},

        {consumer_response, [
            {servers, [
      {{- if eq .Values.global.mq.internal false }}
                  [
                    {host, "${MQ_HOST}"},
                    {port, ${MQ_PORT}},
                    {username, <<"${MQ_USERNAME}">>},
                    {password, <<"${MQ_PASSWORD}">>},
                    {vhost, <<"${MQ_VHOST}">>}
                ]
      {{- else }}
                  [
                    {host, "rabbit-service"},
                    {port, 5672},
                    {username, <<"${MQ_USERNAME}">>},
                    {password, <<"${MQ_PASSWORD}">>},
                    {vhost, <<"${MQ_VHOST}">>}
                ]
      {{- end }}
            ]},
        {connections_per_queue, 1},
        {channels_per_connection, 1},
        {messages_prefetch_size_per_channel, 50}
        ]}
        ]},

        {mq_cache, [
            {publish_request, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
           {queues_count, 1},
           {min_size, 1},
           {max_size, 1},
           {start_size, 1}
          ]},

          {consumer_response, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
          {connections_per_queue, 1},
          {channels_per_connection, 1},
          {messages_prefetch_size_per_channel, 50}
          ]}

        ]},

        {corezoid_queues_gc, [
          {disabled, false},
          {{- if eq .Values.global.mq.internal false }}
          {host, "${MQ_HOST}"},
          {port, 15672 },
          {login, "${MQ_USERNAME}"},
          {password, "${MQ_PASSWORD}"},
          {{- else }}
          {host, "rabbit-service"},
          {port, 15672},
          {login, "${MQ_USERNAME}"},
          {password, "${MQ_PASSWORD}"},
          {{- end }}
          {vhost, "${MQ_VHOST}"},
          {gc_queues_regexp, ["api.ctrl", "settings"]}
        ]},

        {conv_params, [
            {min_version, 2} %% application should deny creating task (copy/rpc) if params are not valid.
                             %% We have in database conveyor table conveyors field version. If version in DB >= min_version in config we run validator else ignore.
        ]},

        %% DEPS for unloading logs to elastic/kibana systems
        %% there are 2 handlegjhrs.
        %% One of them uses for all error messages and crash logs during runtime system works
        %% Rest uses for put callback from direct url and mandrill to kibana/elastic
        %% with masked values (it takes from process autoclear params)
        {corezoid_logs_sender, [
            {handlers, [
                %{error_msg, [                                              %% error messages area
                %{host, ''},                                                %% RabbitMQ host
                %{port, },                                                  %% RabbitMQ port
                %{exchange, <<"CorezoidErrLogs">>},                         %% RabbitMQ exchange
                    %% RabbitMQ queue (param "i" depends on queues_count.
                    %% For queues_count = 4 will be creating 4 queues -
                    %% CorezoidErrLogsQueue1, CorezoidErrLogsQueue2,
                    %% CorezoidErrLogsQueue3, CorezoidErrLogsQueue4
                %{queue, <<"CorezoidErrLogsQueue{{ "{{" }}=i{{ "}}" }}">>},
                %{username, <<"">>},                                        %% RabbitMQ username
                %{password, <<"">>},                                        %% RabbitMQ password
                %{vhost, <<"">>},                                           %% RabbitMQ virtual host
                %{queues_count, 4},                                         %% this parameter is described above
                %{thread_count, 10}                                         %% How many threads will put into queues
            ]}
        ]},
        %% DEPS is used for connect sender and corezoid in one platform.
        %% It replaces liqpay platform
        {merchant_api, [
            %% it uses api for connection and signs all queries using login and secret keys
            {base_url, "http://merchant-service:8080"},         %% url merchant api
            {login,  "${MERCHANT_LOGIN}"},            %% login
            {secret, "${MERCHANT_SECRET}"},
            {skip_otp, true},
            {health_url, "http://merchant-service:7980/actuator/health"}
        ]},

        {enigma, [
          {is_enabled, {{ .Values.global.enigma.enabled }}},
          {private_key_id, "{{ .Values.global.capi.enigma_pk_id }}"},
          {key_manager_host, "http://enigma-key-manager-service:8080"},
          {rotors_pool, [
            {min_size, 3},
            {max_size, 50},
            {start_size, 3}
          ]}
        ]},


        %% MAIN capi application
        {capi,
            [
                %% It must be changed to unique name for every api node. It's personal
                %% queue where will come messages from others api nodes.
                {api_id, <<"">>},
                {server_port, 9080},  %% listener port

                {{- if .Values.global.enigma.encryption }}
                %% for ENCRYPT/DECRYPT data
                {encrypt_decrypt, [
                  queue, %% now it's encode-decode RabbitMQ
                  cache, %% Redis
                  db     %% PostgreSQL
                ]},
                {{- end }}

                {max_task_size_for_process_conv, 264000}, %% max task size for process conv
                {max_task_size_for_st_diagramm_conv, 264000}, %% max task size for state diagramm conv


                %% cookie name where cookie string will put
                {cookie_name, <<"{{ .Values.global.subdomain }}.{{ .Values.global.domain }}">>},
                {shards_count, {{ .Values.global.db.shards_count | default 10 }} },

                %% merchant api module that provides synchronization of the company through the middleware systems (Deepmemo, Corezoid and others)
                {companies_manager, mapi }, %% mapi

                %% max req/sec of create|modify|delete for conv|folder|dashboard
                %% It will be logged as ->
                %% Ops limit is reached. UserId: 1, Obj: conv, Action: create, Limit: 10
                %% End-user will get the error ->
                %% Too many requests: limit is reached
                {max_reqs_limit, {{ .Values.global.capi.max_reqs_limit | default 5 }} },

                %% Main page for output link on dashboard, process, folders and others
                {main_page, "https://{{ .Values.global.subdomain }}.{{ .Values.global.domain }}"},

                %% it is a domain where cookie is binded
                {main_domain, "{{ .Values.global.domain }}"},

                %% It is used for create direct url in viber, telegram link
                %% And for confirm registration
                {api_host, "https://{{ .Values.global.subdomain }}.{{ .Values.global.domain }}/api"},

                %% it is solt for password when we use corezoid auth method(login+password)

                {admin_url1, "https://{{ .Values.global.subdomain }}.{{ .Values.global.domain }}"},       %% First version admin
                {admin_url2, "https://{{ .Values.global.subdomain }}.{{ .Values.global.domain }}"},       %% Second version admin

                %% When new user registers in corezoid system under
                %% login and password it checks this flag
                %% If it is false => corezoid doesn't send confirmation about registration
                %% and bind immediately user to system
                %% If it is true => corezoid send email confirmation and after confirm user
                %% user is binded to corezoid system
                {email_confirm, false},

            %% FRONT SETTINGS
            {front_settings, [
                {is_single_account, {{- .Values.global.sa.enabled }}},	%% enable SingleAccount auth
                {env, <<"prod">>}, %% Available test|pre|prod
                {host, [
                    {site, <<"{{ .Values.global.subdomain }}.{{ .Values.global.domain }}">>},   %% main page (navigate by clicking on the corezoid logo)
                    {doc, <<"{{ .Values.global.capi.front_setting.host.doc }}">>}, %% documentation (navigate by clicking on DOCS link)
                    %%{market, <<"...">>}, %% market (market api call)
                    {ws, <<"{{ .Values.global.subdomain }}.{{ .Values.global.domain }}">>},                    %% websocket
                    {webhook, <<"{{ .Values.global.subdomain }}.{{ .Values.global.domain }}">>},               %% corezoid domain
                    {auth, <<"{{ .Values.global.subdomain }}.{{ .Values.global.domain }}">>},                  %% account auth
                    {single_account, <<"{{ .Values.global.sa_web.subdomain }}.{{ .Values.global.domain }}">>}          %% for single account
                ]},
                {path, [
                    {api, <<"/api/2/json">>},   %% all apis POST queries
                    {upload, <<"/api/2/upload">>},  %% upload scheme, json, csv
                    {download, <<"/api/2/download">>},  %% download scheme, csv
                    {ws, <<"/api/1/sock_json">>},   %% events in real time
            	    {doc, [
                	    {index, <<"/docs">>},
                	    {introduction, <<"/introduction">>},
                	    {bot_platform, <<"/bot-platform-20">>},
                	    {task_export, <<"/tasks-export">>},
                	    {mask_values, <<"/task-parameters#masking-values">>}
            	    ]},
                    {webhook, <<"/api/1/">>},    %% this one plus host.webhook = (http:https)://host.webhook/path.webhook/(xml|json|nvp)/...
                    {auth, <<"/auth2/single_account">>}       %% account auth
                ]},
                {sender, [
                    %% Interaction with the sender to create Sender forms, Sender action...
                    {host, <<"builder.sender.mobi">>},
                    {path, [
                        {embed, <<"/embed.js?">>},
                        {builder, <<"/builder.html">>}
                    ]}
                ]},
                %{captcha_key, <<"">>},
                {captcha, [
                    {key, <<"{{ .Values.global.capi.capi_front_captcha_key | default "" }}">>}, %% key for works with captcha (page /login if corezoid registration)
                    {disabled, {{ .Values.global.capi.capi_front_captcha_key_disabled }} }
                ]},
                {whitelist, [<<"{{ .Values.global.domain }}">>]},
                {ui, [
                    {market, {{ .Values.global.capi.front_setting.ui.market }} },                  %% to Market button
                    {bot_platform, {{ .Values.global.capi.front_setting.ui.bot_platform }} },            %% button Create -> Bot platform
                    {old_editor, false },              %% button Old editor
                    {company, {{ .Values.global.capi.front_setting.ui.company }} },                  %% Button Create -> Company
                    {search, true },                   %% process search
                    {send_invite, true },              %% send an invite or not
                    {health, false },                  %% Show health_check menu
                    {billing, {{ .Values.global.capi.front_setting.ui.billing }} },                                 %% billing button display
                    {git_call, {{ .Values.global.capi.front_setting.ui.git_call }}},                                %% display of the git_call button
                    {tab_name, <<"Corezoid - cloud operating system">>},
                    {disabled_auth_logo, {{ .Values.global.capi.front_setting.ui.disabled_auth_logo }}},            %% disable or enable logo on main page
                    {default_company, << {{ quote .Values.global.capi.front_setting.ui.default_company }} >> }      %% Set default company name
                ]}
            ]},

                %% Elasticsearch includes info:
                %% 1. Processes
                %% 2. Dashboards
                %% 3. Folders
                %% Elastic helps us to find these objects for name, It's as like in DB
            {elastic_search, [
              {{- if eq .Values.global.elasticsearch.internal false }}
                {host, <<"${ELASTICSEARCH_HOST}">>},
                {port, ${ELASTICSEARCH_PORT} },
              {{- else }}
                {host, <<"http://elasticsearch-service">>},
                {port, 9200},
              {{- end }}
                {timeout, 50000}
            ]},

            % PgSQL settings
            %% main database pool settings ( the main base is company folders, processes, i.e. whole front )
            {pgsql, [
          {{- if .Values.global.db.bouncer }}
                {host, "pgbouncer-service"},
                    {hosts, [
                    { [{{- $lastIndex := sub (len .Values.global.db.shards) 1}}
          {{- range $i, $e := .Values.global.db.shards }}
          {{- $i }}{{- if ne $i $lastIndex -}}, {{ end }} {{- end }}], "pgbouncer-service" }
          {{- else }}
                {host, "${POSTGRES_DBHOST}"},
                    %% hosts - tasks, settings of nodes
                    {hosts, [
                    { [{{- $lastIndex := sub (len .Values.global.db.shards) 1}}
          {{- range $i, $e := .Values.global.db.shards }}
          {{- $i }}{{- if ne $i $lastIndex -}}, {{ end }} {{- end }}], "${POSTGRES_DBHOST}" }
          {{- end }}
                    ]},
                {user, "${POSTGRES_DBUSER}"},
                {dbname, "conveyor"},
                {password, "${POSTGRES_DBPWD}"},
                {min_size, 2},          %% The minimum number of connections after the start and 30 seconds of work.
                {max_size, 5},            %% The maximum number of connections is the border to which we can raise, within 30 seconds.
                {start_size, 2}           %% The number of connections that rises to the pool, at the start of this pool. Every 30 seconds check connect and then go to min_size
                %%{max_overflow_pool_size, 50},   %% Legacy: will be removed
                %%{dismiss_overflow, false}       %% Legacy: will be removed
            ]},

            %% database pool for for usercode sandboxes only
            {pgsql_cce_temp, [
          {{- if .Values.global.db.bouncer }}
                {host, "pgbouncer-service"},
          {{- else }}
                {host, "${POSTGRES_DBHOST}"},
          {{- end }}
                {user, "${POSTGRES_DBUSER}"},
                {db_name, "cce"},
                {password, "${POSTGRES_DBPWD}"},
                {min_size, 2},
                {max_size, 25},
                {start_size, 2}
            ]},

            %% database for git call
            {pgsql_git_call, [
          {{- if .Values.global.db.bouncer }}
                {host, "pgbouncer-service"},
          {{- else }}
                {host, "${POSTGRES_DBHOST}"},
          {{- end }}
                {user, "${POSTGRES_DBUSER}"},
                {db_name, "git_call"},
                {password, "${POSTGRES_DBPWD}"},
                {min_size, 0},
                {max_size, 0},
                {start_size, 0}
            ]},

          %%  postgresql archive db
          { pgsql_archive, [
            {{- if .Values.global.db.bouncer }}
              { hosts, [
                { [{{- $lastIndex := sub (len .Values.global.db.shards) 1}}
            {{- range $i, $e := .Values.global.db.shards }}
            {{- $i }}{{- if ne $i $lastIndex -}}, {{ end }} {{- end }}], "pgbouncer-service" }
              ]},
            {{- else }}
              { hosts, [
                { [{{- $lastIndex := sub (len .Values.global.db.shards) 1}}
            {{- range $i, $e := .Values.global.db.shards }}
            {{- $i }}{{- if ne $i $lastIndex -}}, {{ end }} {{- end }}], "${POSTGRES_DBHOST}" }
              ]},
            {{- end }}
              { user, "${POSTGRES_DBUSER}" },
              { password, "${POSTGRES_DBPWD}" },
              { start_size, 2 },
              { min_size, 2 },
              { max_size, 50 }
            ]},



            %% database pool for highloads clients (removal of highly loaded processes in a separate database)
            %% Extended solution
            {pgsql_extra, []},

            {kernel, [
                %%{inet_dist_listen_min, 52617},          %% for api cluster the minimum port that can use
                %%{inet_dist_listen_max, 52617}           %% for api cluster the maximum port that can use
            ]},

            % redis pool for counters and api-sum-s
            {redis1, [
                [   {{- if eq .Values.global.redis.internal false }}
                    {host, "${REDIS_HOST}"},
                    {port, ${REDIS_PORT}},
                    {password,"${REDIS_PASSWORD}"},
                    {{- else }}
                    {host, "redis-master"},
                    {port, 6379},
                    {password,""},
                    {{- end }}
                    {database,1},
                    {start_size, 5},
                    {min_size, 5},
                    {max_size, 100}
                ]
            ]},

            %% memory redis for cache task
            {redis2, [
                [   {{- if eq .Values.global.redis.internal false }}
                    {host, "${REDIS_HOST}"},
                    {port, ${REDIS_PORT}},
                    {password,"${REDIS_PASSWORD}"},
                    {{- else }}
                    {host, "redis-master"},
                    {port, 6379},
                    {password,""},
                    {{- end }}
                    {database,3},
                    {start_size, 10},
                    {min_size, 10},
                    {max_size, 200}
                ]
            ]},

            %% redis pool for api_sum logic
            {redis_api_sum, [
                [   {{- if eq .Values.global.redis.internal false }}
                    {host, "${REDIS_HOST}"},
                    {port, ${REDIS_PORT}},
                    {password,"${REDIS_PASSWORD}"},
                    {{- else }}
                    {host, "redis-master"},
                    {port, 6379},
                    {password,""},
                    {{- end }}
                    {database,2},
                    {start_size, 2},
                    {min_size, 2},
                    {max_size, 50}
                ]
            ]},

            % to_worker mq
            %% (For scaling, the workers can communicate with different rabbitmqs (for example, 1 worker serves 1-5 shards, the 2nd worker serves 6-10 shards. They know who serves what among themselves))
            {publish_to_worker_request, [
                {servers, [
                    { [{{- $lastIndex := sub (len .Values.global.db.shards) 1}}
          {{- range $i, $e := .Values.global.db.shards }}
          {{- $i }}{{- if ne $i $lastIndex -}}, {{ end }} {{- end }}], [
          {{- if eq .Values.global.mq.internal false }}
                    {host, "${MQ_HOST}"}
                    ]}
                ]},
                {port, ${MQ_PORT} },
                {username, <<"${MQ_USERNAME}">>},
                {password, <<"${MQ_PASSWORD}">>},
          {{- else }}
                        {host, "rabbit-service"}
                    ]}
                ]},
                {port, 5672},
                {username, <<"${MQ_USERNAME}">>},
                {password, <<"${MQ_PASSWORD}">>},
          {{- end }}
                {vhost, <<"${MQ_VHOST}">>},
                {min_size, 5},
                {max_size, 5},
                {start_size, 5}
            ]},

            % api copy queue (support multiply consumers)
            {consumer_copy_task_request,[
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {queues_count, 1},
                {connections_per_queue, 1},
                {channels_per_connection, 1},
                {messages_prefetch_size_per_channel, 50}
            ]},

            %% logic get_task. Now on api will soon be on worker
            % api get_task queue (support multiply consumers)
            {consumer_get_task_request,[
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {order_by, true},
                {queues_count, 1},
                {connections_per_queue, 1},
                {channels_per_connection, 1},
                {messages_prefetch_size_per_channel, 50}
            ]},

            %% async events queue between users publisher
            {publish_user_actions_request, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {queues_count, 1},
                {min_size, 1},
                {max_size, 1},
                {start_size, 1}
            ]},

            %% async events queue consumer
            {consumer_user_actions_request, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {queues_count, 1},
                {connections_per_queue, 1},
                {channels_per_connection, 1},
                {messages_prefetch_size_per_channel, 50}
            ]},

            %% deprecated
            {consumer_notify_actions_request, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {queues_count, 1},
                {connections_per_queue, 1},
                {channels_per_connection, 2},
                {messages_prefetch_size_per_channel, 50}
            ]},

            %% consumer for multipart-worker
            %% work in pair with multipart worker
            %% through this queue goes tasks with loading scheme from multipart to api
            {consumer_multipart_connector_request, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {queues_count, 1},
                {connections_per_queue, 1},
                {channels_per_connection, 2},
                {messages_prefetch_size_per_channel, 50}
            ]},

            %% elasticsearch consumer
            {consumer_elastic_actions_request, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {queues_count, 1},
                {connections_per_queue, 1},
                {channels_per_connection, 2},
                {messages_prefetch_size_per_channel, 50}
            ]},

            %% settings publisher
            {publish_settings, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {min_size, 1},
                {max_size, 1},
                {start_size, 1}
            ]},

            %% settings consumer
            {consumer_settings, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {connections_per_queue, 1},
                {channels_per_connection, 1},
                {messages_prefetch_size_per_channel, 50}
            ]},



            %% Statistics consumer
            {consumer_statistics, [
                {servers, [
          {{- if eq .Values.global.mq.internal false }}
                      [
                        {host, "${MQ_HOST}"},
                        {port, ${MQ_PORT}},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- else }}
                      [
                        {host, "rabbit-service"},
                        {port, 5672},
                        {username, <<"${MQ_USERNAME}">>},
                        {password, <<"${MQ_PASSWORD}">>},
                        {vhost, <<"${MQ_VHOST}">>}
                    ]
          {{- end }}
                ]},
                {connections_per_queue, 1},
                {channels_per_connection, 1},
                {messages_prefetch_size_per_channel, 1}
            ]},

            % ldap auth settings
            {ldap, [
                {server, "{{ .Values.global.capi.ldap_server }}"},
                {port, {{ .Values.global.capi.ldap_port }} },
          {{- if and .Values.global.capi.ldap_tls (eq .Values.global.capi.ldap_tls true) }}
                {tls, {{ .Values.global.capi.ldap_tls }}}, %% true | false
          {{- end }}
                {base, "{{ .Values.global.capi.ldap_base }}"},      %% ou=special users,o=middleware
                {filter, "{{ .Values.global.capi.ldap_filter }}"}, %% uid | cn
                {first_bind_user, {{ .Values.global.capi.ldap_first_bind_user }} }, %% then this param is true, bind_user_name, bind_user_pass should be filled. if it's false it is not necessary
                {bind_user_name, "{{ .Values.global.capi.ldap_bind_user_name }}" }, %% or like this "cn=middleware,ou=DHO,ou=fuib,dc=fuib,dc=com"
                {bind_user_pass, "{{ .Values.global.capi.ldap_bind_user_pass }}" },
                {user_nick_entry, "{{ .Values.global.capi.ldap_user_nick_entry }}" } %% ldap nick name path
            ]},

            %% google auth settings
            {oauth, [
              %% google auth for {{ .Values.global.subdomain }}.{{ .Values.global.domain }} (api1)
              {auth_google, [
                {client_id, "{{ .Values.global.sa.google_client_id }}"},
                {client_secret, "{{ .Values.global.sa.google_client_secret }}"},
                {return_url, "https://{{ .Values.global.subdomain }}.{{ .Values.global.domain }}/auth/google/return"},
                {type, auth_google},
                {oauth_url, "https://accounts.google.com/o/oauth2/auth"},
                {token_url, "https://accounts.google.com/o/oauth2/token"},
                {userinfo_url, "https://www.googleapis.com/oauth2/v1/userinfo?access_token="},
                {status, on}
              ]},

              %% google auth for {{ .Values.global.subdomain }}.{{ .Values.global.domain }} (api2)
              {auth2_google, [
                {client_id, "{{ .Values.global.sa.google_client_id }}"},
                {client_secret, "{{ .Values.global.sa.google_client_secret }}"},
                {return_url, "https://{{ .Values.global.subdomain }}.{{ .Values.global.domain }}/auth2/google/return"},
                {type, auth_google},
                {oauth_url, "https://accounts.google.com/o/oauth2/auth"},
                {token_url, "https://accounts.google.com/o/oauth2/token"},
                {userinfo_url, "https://www.googleapis.com/oauth2/v1/userinfo?access_token="},
                {status, on}
              ]}

            ]},


            %% Setting captcha backend
            {backend_settings, [
                {captcha, [
                    {key, <<"{{ .Values.global.capi.capi_backend_captcha_key }}">>},
                    {verify_url, "https://www.google.com/recaptcha/api/siteverify"},
                    {disabled, {{ .Values.global.capi.capi_backend_captcha_key_disabled }} }
                ]}
            ]},

            %% sending metrics to zabbix
            {zabbix, [
                {server, "localhost"},
                {src_host, "corezoid"},
                {send_interval, 5},
                {disabled, true}
            ]},

            {sender, [
                %% sender communication
                {sender_build_form_url, "https://api.sender.mobi"}, %% for build form url
                {sender_build_action_url, "https://api-adm.sender.mobi"}, %% for action url
                {sender_call_action_url, "https://api-conv.sender.mobi"}, %% for call action url
                {sender_secret, <<"">>},
                {sender_plugin_secret, <<"">>},
                {sender_max_threads, 25 },
                {sender_env, <<"md">>}
            ]},

            %% {allowed_domains, ["gmail.com", "corezoid.com", "github.com"]},

            % api limit counters by user_id
            {user_limits, [
                {max_interface_rate, {{ .Values.global.capi.user_limits.max_interface_rate | default 100 }} },  %% default limit interface requests, ban after for 1 min
                {max_user_rate, {{ .Values.global.capi.user_limits.max_user_rate | default 2000 }} }         %% default limit for task create/modify, other will get 429 error
            ]},

            {logic_settings, [
                {api, [
                    {max_threads, {{ .Values.global.capi.logic_settings.api_max_thread | default 200 }}}
                ]},
                {sender_api, [
                    {max_threads, {{ .Values.global.capi.logic_settings.api_max_thread | default 25 }}}
                ]},
                {timer, [
                    {default, [
                        {timer_min, {{ .Values.global.capi.logic_settings.timer_default | default 30 }}}
                    ]}
                ]}
            ]}
        ]},

        %% sending metrics to zabbix
        {zabbix_sender, [
            {zabbix_host, "localhost"},
            {zabbix_port, 10051},
            {nodename, "corezoid"},
            {disabled, true}
        ]},

        {lager, [
            %% What handlers to install with what arguments

            {log_root, "/ebsmnt/erlang/capi/log"},
            {handlers, [
                {lager_console_backend, info},
                {lager_file_backend, [{file, "error.log"}, {level, error}, {size, 734003200}, {date, "$D0"}, {count, 1}]} %%,
                %%{lager_file_backend, [{file, "console.log"}, {level, info}, {size, 734003200}, {date, "$D0"}, {count, 1}]}
            ]},
            %% What colors to use with what log levels
            {colored, true},
            {colors, [
                {debug,     "\e[0;38m" },
                {info,      "\e[1;37m" },
                {notice,    "\e[1;36m" },
                {warning,   "\e[1;33m" },
                {error,     "\e[1;31m" },
                {critical,  "\e[1;35m" },
                {alert,     "\e[1;44m" },
                {emergency, "\e[1;41m" }
            ]},
            %% Whether to write a crash log, and where. Undefined means no crash logger.
            {crash_log, "crash.log"},
            %% Maximum size in bytes of events in the crash log - defaults to 65536
            {crash_log_msg_size, 65536},
            %% Maximum size of the crash log in bytes, before its rotated, set
            %% to 0 to disable rotation - default is 0
            {crash_log_size, 10485760},
            %% What time to rotate the crash log - default is no time
            %% rotation. See the README for a description of this format.
            {crash_log_date, "$D0"},
            %% Number of rotated crash logs to keep, 0 means keep only the
            %% current one - default is 0
            {crash_log_count, 5},
            %% Whether to redirect error_logger messages into lager - defaults to true
            {error_logger_redirect, true},
            %% How many messages per second to allow from error_logger before we start dropping them
            {error_logger_hwm, 50},
            %% How big the gen_event mailbox can get before it is switched into sync mode
            {async_threshold, 20},
            %% Switch back to async mode, when gen_event mailbox size decrease from `async_threshold'
            %% to async_threshold - async_threshold_window
            {async_threshold_window, 5}
        ]},

        {hcheck_sender, [
            {host, <<"hcs-service">>}, %% host of the remote healthcheck server
            {port, 5011}, %% port of the remote healthcheck server
            {node_name, <<"capi-{{ .Values.global.subdomain }}.{{ .Values.global.domain }}">> }, %% different for each node
            {node_type, <<"capi">> }, %% capi | worker | multipart | http_worker | usercode | deepmemo ...
            {disabled, true}, %% true by default
            {send_interval_sec, 30}, %% by default 10 sec
            {send_system_counters, true} %% memory processes etc, false by default
        ]},

        %% SASL config
        {sasl, [
            {sasl_error_logger, {file, "log/sasl-error.log"}},
            {errlog_type, error},
            {error_logger_mf_dir, "log/sasl"},      % Log directory
            {error_logger_mf_maxbytes, 10485760},   % 10 MB max file size
            {error_logger_mf_maxfiles, 5}           % 5 files max
        ]},

        {account_sdk, [
        {{- if not .Values.global.sa.enabled }}
            {disabled, true}
        {{- else }}
            {pool, [
                {host, "{{ .Values.global.sa_web.subdomain }}.{{ .Values.global.domain }}"},
                {port, 443},
                {start_size, 10},
                {min_size, 10},
                {max_size, 10}
            ]},
            {disabled, false},
            {client_id, "5dca7de57837d70001000006"},
            {client_secret, "ZY8bwLJDYouuApi2ENxlup6kqShnhk3U"},
            {return_url, "https://{{ .Values.global.subdomain }}.{{ .Values.global.domain }}/auth2/single_account/return/"},
            {oauth_url, "https://{{ .Values.global.sa_web.subdomain }}.{{ .Values.global.domain }}/oauth2/authorize"},
            {token_url, "https://{{ .Values.global.sa_web.subdomain }}.{{ .Values.global.domain }}/oauth2/token"},
            {userinfo_url, "https://{{ .Values.global.sa_web.subdomain }}.{{ .Values.global.domain }}/oauth2/userinfo?access_token="}
        {{- end }}
        ]}

        ].

kind: ConfigMap
metadata:
  name: {{ .Values.appName }}-config
